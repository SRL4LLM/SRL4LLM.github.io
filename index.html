<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Structured Representation Learning Tutorial</title>
    <link rel="icon" type="image/png" href="files/favicon.png">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bulma@0.9.4/css/bulma.min.css">
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <nav class="navbar is-transparent">
        <div class="container">
            <div class="navbar-end">
                <div class="navbar-item">
                    <a href="/" class="nav-link">Home</a>
                </div>
                <div class="navbar-item">
                    <a href="/reading-list.html" class="nav-link">Reading List</a>
                </div>
                
                <div class="navbar-item">
                    <a href="files/AAAI26-RepLearning-updating.pdf" class="nav-link" target="_blank">Tutorial PDF</a>
                </div>


            </div>
        </div>
    </nav>
    <section class="hero">
        <div class="container">
                <h1 class="main-title has-text-centered">
                    Structured Representation Learning
                </h1>
                <h2 class="secondary-title has-text-centered">
                    Interpretability, Robustness, and Transferability for LLMs
                </h2>
                <h3 class="conference-title has-text-centered">
                    AAAI 2026 Tutorial
                </h3>
                
                    <!-- Time & Place -->
                <div class="time-place has-text-centered" style="margin:20px 0;">
                  <h3 style="font-weight:700; color:#CC4B4B; margin:0;">
                    Time: <strong> Tuesday, January 20, 2:00–6:00 PM </strong> &nbsp; | &nbsp; 
                    Place: <strong>Peridot 205,Singapore EXPO</strong>
                  </h3>
                </div>

                <div class="author-headshots">
                    <div class="headshot-container">
                        <img src="files/headshots/hanqi.png" alt="Hanqi Yan" class="headshot">
                        <p class="headshot-name">Hanqi Yan</p>
                        <p class="headshot-affiliation">King’s College London</p>
                    </div>
                    <div class="headshot-container">
                        <img src="files/headshots/cgy.jpg" alt="Guangyi Chen" class="headshot">
                        <p class="headshot-name">Guangyi Chen</p>
                        <p class="headshot-affiliation">CMU & MBZUAI</p>
                    </div>
                    <div class="headshot-container">
                        <img src="files/headshots/jonathan.jpg" alt="Jonathan Richard Schwarz" class="headshot">
                        <p class="headshot-name">Jonathan Richard Schwarz</p>
                        <p class="headshot-affiliation">ICL & Thomson Reuters</p>
                    </div>
                </div>
                <div class="short-abstract">
                    <p><strong>Abstract.</strong> As LLMs move from research labs to real-world applications, understanding and controlling their behavior has become critical, given their rapid evolution and opaque internal mechanisms. This tutorial explores principled representation learning as a foundation for controllability, interpretability, and transferability. Participants will learn how to build interpretable, modular representations that guide model behavior, improve reasoning efficiency, and extend capabilities to new tasks through recombination. Grounded in human-centered measures and careful data design, this tutorial offers a roadmap for more robust, transparent, and trustworthy LLM-assisted systems. </p>
                </div>
                <!-- <div class="abstract">
                    <h3>Abstract</h3>
                    <p>Since the inception of language models (LLMs), considerable attention has been directed toward the field of AI safety. These efforts aim to identify a range of best practices—including evaluation protocols, defense algorithms, and content filters—that facilitate the ethical, trustworthy, and reliable deployment of LLMs and related technologies. A key component of AI safety is model alignment, a broad concept referring to algorithms that optimize the outputs of LLMs to align with human values. And yet, despite these efforts, recent research has identified several failure modes—referred to as jailbreaks—that circumvent LLM alignment by eliciting unsafe content from a targeted model. And while initial jailbreaks targeted the generation of harmful information (e.g., copyrighted or illegal material), modern attacks seek to elicit domain-specific harms, such as digital agents violating user privacy and LLM-controlled robots performing harmful actions in the physical world. In the worst case, future attacks may target self-replication or power seeking behaviors. The insidious nature of jailbreaking attacks represents a substantial obstacle to the broad adoption of LLMs. Therefore, it is critical for the machine learning community to study these failure modes and develop effective defense strategies that counteract them.</p>

                    <p>Over the past two years, research in both academia and industry has sought to design new attacks that stress test model safeguards, and to develop stronger defenses against these attacks. And, in general, this concerted work has resulted in safer models. Notably, highly performant LLMs such as OpenAI's o-series and Anthropic's Claude3 models demonstrate significant robustness against a variety of jailbreaking attacks. However, the evolving arms race between jailbreaking attacks and defenses demonstrates that meeting acceptable standards of safety remains a work in progress. To provide a comprehensive overview of the evolving landscape in this field, this tutorial aims to present a unified perspective on recent progress in the jailbreaking community. In line with this goal, the primary objectives of this tutorial are as follows: (i) We will review <em><strong>cutting-edge advances</strong></em> in jailbreaking, covering new algorithmic frameworks and mathematical foundations, with particular emphasis on attack, defenses, evaluations, and applications in robotics and agentic systems; (ii) Noting that the foundations of jailbreaking are still at their infancy, we will discuss the plethora of <em><strong>new directions, opportunities, and challenges</strong></em> that have recently been brought to bear by the identification of jailbreaking attacks; (iii) We will walk through a range of <em><strong>open-source Python implementations</strong></em> of state-of-the-art algorithms.</p>
                </div> -->
        </div>
    </section>

    <footer class="footer">
        <div class="content has-text-centered">
            <p>
                <strong> Structured Representation Learning for Large Language Models </strong> - AAAI 2026 Tutorial
            </p>
        </div>
    </footer>
</body>
</html> 